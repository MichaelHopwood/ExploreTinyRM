{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import exploretinyrm as m\n",
    "m.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.10167226195335388,\n",
       " 'standard_deviation': 0.8768734931945801,\n",
       " 'minimum': -1.5688676834106445,\n",
       " 'maximum': 1.3839704990386963}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional) make notebooks pick up code edits automatically without kernel restarts:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from exploretinyrm.utils import compute_tensor_summary\n",
    "\n",
    "test_tensor = torch.randn(5, 3)\n",
    "summary = compute_tensor_summary(test_tensor)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExploreTinyRM version: 0.1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import exploretinyrm as etm\n",
    "from exploretinyrm import TRM, TRMConfig\n",
    "\n",
    "print(\"ExploreTinyRM version:\", etm.__version__)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters (attention TRM): 164,288\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 32\n",
    "config_attn = TRMConfig(\n",
    "    input_vocab_size=256,\n",
    "    output_vocab_size=256,\n",
    "    seq_len=sequence_length,\n",
    "    d_model=64,\n",
    "    n_layers=2,\n",
    "    use_attention=True,   # self-attention path\n",
    "    n_heads=4,\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=2,                  # inner loops\n",
    "    T=2,                  # deep-recursion loops per supervision step\n",
    "    k_last_ops=None       # backprop through all net-calls in final recursion\n",
    ")\n",
    "\n",
    "model_attn = TRM(config_attn).to(device)\n",
    "total_parameters = sum(p.numel() for p in model_attn.parameters())\n",
    "print(f\"Parameters (attention TRM): {total_parameters:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_next: torch.Size([2, 32, 64]) requires_grad: False\n",
      "z_next: torch.Size([2, 32, 64]) requires_grad: False\n",
      "logits: torch.Size([2, 32, 256])\n",
      "halt_logit: torch.Size([2])\n",
      "Basic forward + shapes OK.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "x_tokens = torch.randint(low=0, high=config_attn.input_vocab_size,\n",
    "                         size=(batch_size, sequence_length), device=device)\n",
    "\n",
    "# optional: explicit state init (the API will do this if you pass None)\n",
    "y_state, z_state = model_attn.init_state(batch_size=batch_size, device=device)\n",
    "\n",
    "y_next, z_next, logits, halt_logit = model_attn.forward_step(\n",
    "    x_tokens, y_state, z_state\n",
    ")\n",
    "\n",
    "print(\"y_next:\", y_next.shape, \"requires_grad:\", y_next.requires_grad)\n",
    "print(\"z_next:\", z_next.shape, \"requires_grad:\", z_next.requires_grad)\n",
    "print(\"logits:\", logits.shape)\n",
    "print(\"halt_logit:\", halt_logit.shape)\n",
    "\n",
    "assert y_next.shape == (batch_size, sequence_length, config_attn.d_model)\n",
    "assert z_next.shape == (batch_size, sequence_length, config_attn.d_model)\n",
    "assert logits.shape == (batch_size, sequence_length, config_attn.output_vocab_size)\n",
    "assert halt_logit.shape == (batch_size,)\n",
    "assert y_next.requires_grad is False and z_next.requires_grad is False  # states are detached\n",
    "print(\"Basic forward + shapes OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with nonzero grads: 15\n",
      "Backward pass OK.\n"
     ]
    }
   ],
   "source": [
    "# simple supervised loss on logits against random targets\n",
    "target_tokens = torch.randint(\n",
    "    low=0, high=config_attn.output_vocab_size,\n",
    "    size=(batch_size, sequence_length), device=device\n",
    ")\n",
    "loss = F.cross_entropy(\n",
    "    logits.view(-1, config_attn.output_vocab_size),\n",
    "    target_tokens.view(-1)\n",
    ")\n",
    "loss.backward()\n",
    "\n",
    "# confirm some gradients exist\n",
    "num_with_grads = sum((p.grad is not None) and (p.grad.abs().sum() > 0) for p in model_attn.parameters())\n",
    "print(f\"Parameters with nonzero grads: {num_with_grads}\")\n",
    "assert num_with_grads > 0\n",
    "model_attn.zero_grad(set_to_none=True)\n",
    "print(\"Backward pass OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP mixer run OK: torch.Size([2, 16, 64]) torch.Size([2, 16, 64]) torch.Size([2, 16, 128]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "sequence_length_mlp = 16  # mixer is best for short, fixed L; keep it small here\n",
    "config_mlp = TRMConfig(\n",
    "    input_vocab_size=128,\n",
    "    output_vocab_size=128,\n",
    "    seq_len=sequence_length_mlp,\n",
    "    d_model=64,\n",
    "    n_layers=2,\n",
    "    use_attention=False,  # token-MLP path\n",
    "    n_heads=4,            # ignored when use_attention=False\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=2,\n",
    "    T=2,\n",
    "    k_last_ops=None\n",
    ")\n",
    "\n",
    "model_mlp = TRM(config_mlp).to(device)\n",
    "x_tokens_mlp = torch.randint(0, config_mlp.input_vocab_size,\n",
    "                             (batch_size, sequence_length_mlp), device=device)\n",
    "\n",
    "y_next2, z_next2, logits2, halt_logit2 = model_mlp.forward_step(x_tokens_mlp)\n",
    "print(\"MLP mixer run OK:\",\n",
    "      y_next2.shape, z_next2.shape, logits2.shape, halt_logit2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated-grad test OK (k_last_ops=2).\n"
     ]
    }
   ],
   "source": [
    "# Keep gradients only through the last 2 \"net calls\" of the final recursion.\n",
    "# Each inner loop performs 2 net calls (z update, then y update).\n",
    "config_trunc = TRMConfig(**{**config_attn.__dict__, \"k_last_ops\": 2})\n",
    "model_trunc = TRM(config_trunc).to(device)\n",
    "\n",
    "x_tokens_t = torch.randint(0, config_trunc.input_vocab_size,\n",
    "                           (batch_size, sequence_length), device=device)\n",
    "y_t, z_t, logits_t, halt_t = model_trunc.forward_step(x_tokens_t, k_last_ops=2)\n",
    "\n",
    "loss_t = logits_t.pow(2).mean()  # any scalar loss\n",
    "loss_t.backward()\n",
    "print(\"Truncated-grad test OK (k_last_ops=2).\")\n",
    "model_trunc.zero_grad(set_to_none=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
