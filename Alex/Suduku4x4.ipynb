{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import exploretinyrm as m\n",
    "print(m.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from exploretinyrm.utils import compute_tensor_summary  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meow/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from exploretinyrm.trm import TRM, TRMConfig\n",
    "\n",
    "def set_seed(seed: int = 123):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AMP setup (PyTorch 2.x forward-compatible) ---\n",
    "\n",
    "# Prefer the new torch.amp API; fall back only if needed.\n",
    "try:\n",
    "    from torch.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "    _USE_TORCH_AMP = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "    _USE_TORCH_AMP = False\n",
    "\n",
    "def make_grad_scaler(is_cuda: bool):\n",
    "    # New API: GradScaler(\"cuda\", enabled=...)\n",
    "    if _USE_TORCH_AMP:\n",
    "        try:\n",
    "            return _GradScaler(\"cuda\", enabled=is_cuda)\n",
    "        except TypeError:\n",
    "            # Older torch.amp without device_type arg\n",
    "            return _GradScaler(enabled=is_cuda)\n",
    "    else:\n",
    "        # Legacy cuda.amp API\n",
    "        return _GradScaler(enabled=is_cuda)\n",
    "\n",
    "def amp_autocast(is_cuda: bool, use_amp: bool):\n",
    "    # New API: autocast(device_type=\"cuda\", enabled=...)\n",
    "    if _USE_TORCH_AMP:\n",
    "        try:\n",
    "            return _autocast(device_type=\"cuda\", enabled=(is_cuda and use_amp))\n",
    "        except TypeError:\n",
    "            # Older torch.amp without device_type arg\n",
    "            return _autocast(enabled=(is_cuda and use_amp))\n",
    "    else:\n",
    "        # Legacy cuda.amp API\n",
    "        return _autocast(enabled=(is_cuda and use_amp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EMA utility (training-time only) ---\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: torch.nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        # Track only trainable parameters\n",
    "        self.shadow = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in model.named_parameters()\n",
    "            if param.requires_grad\n",
    "        }\n",
    "\n",
    "    def update(self, model: torch.nn.Module) -> None:\n",
    "        d = self.decay\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if not param.requires_grad:\n",
    "                    continue\n",
    "                self.shadow[name].mul_(d).add_(param.detach(), alpha=1.0 - d)\n",
    "\n",
    "    def copy_to(self, model: torch.nn.Module) -> None:\n",
    "        # Overwrite model parameters with EMA weights\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in self.shadow:\n",
    "                    param.copy_(self.shadow[name])\n",
    "\n",
    "\n",
    "\n",
    "# A small context manager to temporarily evaluate with EMA weights\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def use_ema_weights(model: torch.nn.Module, ema: EMA):\n",
    "    # Save current weights, load EMA, run, then restore\n",
    "    backup = {\n",
    "        name: param.detach().clone()\n",
    "        for name, param in model.named_parameters()\n",
    "        if param.requires_grad\n",
    "    }\n",
    "    ema.copy_to(model)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in backup:\n",
    "                    param.copy_(backup[name])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset (puzzle, solution)\n",
    "\n",
    "SIDE = 4\n",
    "BASE = 2\n",
    "INPUT_PAD = 0               # 0 marks blank in the INPUT ONLY\n",
    "INPUT_TOKENS = SIDE + 1     # {0..4} for inputs\n",
    "OUTPUT_TOKENS = SIDE        # {0..3} for outputs (represents digits 1..4)\n",
    "\n",
    "BASE_SOLUTION = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [3, 4, 1, 2],\n",
    "    [2, 1, 4, 3],\n",
    "    [4, 3, 2, 1],\n",
    "], dtype=np.int64)\n",
    "\n",
    "def permute_solution(board: np.ndarray, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"random legal permutations: rows/cols within bands/stacks, swap bands/stacks, digit perm\"\"\"\n",
    "    b = BASE; s = SIDE\n",
    "    # rows within bands\n",
    "    row_idx = []\n",
    "    bands = [list(range(g*b, (g+1)*b)) for g in range(b)]\n",
    "    for band in bands:\n",
    "        rng.shuffle(band); row_idx.extend(band)\n",
    "    board = board[row_idx, :]\n",
    "    # cols within stacks\n",
    "    col_idx = []\n",
    "    stacks = [list(range(g*b, (g+1)*b)) for g in range(b)]\n",
    "    for stack in stacks:\n",
    "        rng.shuffle(stack); col_idx.extend(stack)\n",
    "    board = board[:, col_idx]\n",
    "    # swap bands\n",
    "    band_order = list(range(b)); rng.shuffle(band_order)\n",
    "    row_idx = []\n",
    "    for g in band_order: row_idx.extend(list(range(g*b, (g+1)*b)))\n",
    "    board = board[row_idx, :]\n",
    "    # swap stacks\n",
    "    stack_order = list(range(b)); rng.shuffle(stack_order)\n",
    "    col_idx = []\n",
    "    for g in stack_order: col_idx.extend(list(range(g*b, (g+1)*b)))\n",
    "    board = board[:, col_idx]\n",
    "    # permute digits 1..SIDE\n",
    "    digits = np.arange(1, s+1); rng.shuffle(digits)\n",
    "    mapping = {i+1: digits[i] for i in range(s)}\n",
    "    return np.vectorize(lambda v: mapping[v])(board)\n",
    "\n",
    "def make_puzzle(solution: np.ndarray, p_blank: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"mask entries with probability p_blank to form the puzzle\"\"\"\n",
    "    mask = rng.random(solution.shape) < p_blank\n",
    "    puzzle = solution.copy()\n",
    "    puzzle[mask] = INPUT_PAD\n",
    "    return puzzle\n",
    "\n",
    "\n",
    "class Sudoku4x4(Dataset):\n",
    "    def __init__(self, n_samples: int, p_blank: float = 0.5, seed: int = 0):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.samples = []\n",
    "        for _ in range(n_samples):\n",
    "            sol = permute_solution(BASE_SOLUTION, self.rng)     # digits in 1..4\n",
    "            puz = make_puzzle(sol, p_blank=p_blank, rng=self.rng)\n",
    "            x_tokens = puz.reshape(-1).astype(np.int64)         # [16], values in {0..4}\n",
    "            y_digits = sol.reshape(-1).astype(np.int64)         # [16], values in {1..4}\n",
    "            y_tokens = (y_digits - 1)                           # map to {0..3} for CE\n",
    "            self.samples.append((x_tokens, y_tokens))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "def get_loaders(n_train=512, n_val=128, batch_size=16, p_blank=0.45, seed=123):\n",
    "    # slightly easier p_blank for the first sanity run; you can return to 0.50 after it learns\n",
    "    ds_tr = Sudoku4x4(n_train, p_blank=p_blank, seed=seed)\n",
    "    ds_va = Sudoku4x4(n_val,   p_blank=p_blank, seed=seed+1)\n",
    "    return (\n",
    "        DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True),\n",
    "        DataLoader(ds_va, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    )\n",
    "\n",
    "train_loader, val_loader = get_loaders(\n",
    "    n_train=2048, #4096\n",
    "    n_val=512,\n",
    "    batch_size=16,\n",
    "    p_blank=0.50,\n",
    "    seed=123\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity OK: x in [0,SIDE], y in [0,SIDE-1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ensure labels are in 0 to 3 and inputs are in 0to4\n",
    "bx, by = next(iter(train_loader))\n",
    "assert bx.min().item() >= 0 and bx.max().item() <= SIDE\n",
    "assert by.min().item() >= 0 and by.max().item() < SIDE\n",
    "print(\"Sanity OK: x in [0,SIDE], y in [0,SIDE-1]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params (M): 0.397312\n"
     ]
    }
   ],
   "source": [
    "# keep SIDE, SEQ_LEN, USE_ATT as before\n",
    "D_MODEL = 128\n",
    "SEQ_LEN = SIDE * SIDE\n",
    "N_SUP   = 16          # paper uses \"up to 16\" for Sudoku\n",
    "N       = 6  # of z-updates inside a recursion process\n",
    "T       = 3  # of full recursion processes per supervision step\n",
    "USE_ATT = False # false for SUDUKU ONLY !!!!!!!\n",
    "\n",
    "\n",
    "cfg = TRMConfig(\n",
    "    input_vocab_size=INPUT_TOKENS,  # 5\n",
    "    output_vocab_size=OUTPUT_TOKENS,  # 4\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=2,   \n",
    "    use_attention=USE_ATT, # token MLP for SUDUKU\n",
    "    n_heads=8,        # ignored when use_attention=False; harmless but see note below\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=N,\n",
    "    T=T,\n",
    "    k_last_ops=None,\n",
    "    stabilize_input_sums = True #False to be like the paper but here produces NaN !!!\n",
    ")\n",
    "\n",
    "\n",
    "model = TRM(cfg).to(device)\n",
    "print(\"Params (M):\", sum(p.numel() for p in model.parameters())/1e6)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=3e-4, weight_decay=0.0, betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "scaler = make_grad_scaler(device.type == \"cuda\")\n",
    "\n",
    "#initialize EMA after model and optimizer exist\n",
    "ema = EMA(model, decay=0.999)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_ema(model: TRM, ema: EMA, loader: DataLoader, n_sup_eval: int = N_SUP):\n",
    "    with use_ema_weights(model, ema):\n",
    "        return evaluate(model, loader, n_sup_eval=n_sup_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "def exact_match_from_logits(logits: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    # logits: [B, L, V], y_true: [B, L]\n",
    "    preds = logits.argmax(dim=-1)                      # [B, L]\n",
    "    return (preds == y_true).all(dim=1).float()        # [B]\n",
    "\n",
    "def token_ce_loss(logits: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    # flatten to (B*L, V) vs (B*L)\n",
    "    B, L, V = logits.shape\n",
    "    return F.cross_entropy(logits.reshape(B*L, V), y_true.reshape(B*L))\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: TRM,\n",
    "    loader: DataLoader,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    epoch: int,\n",
    "    use_amp: bool = True,\n",
    "    ema: \"EMA | None\" = None\n",
    "):\n",
    "    model.train()\n",
    "    total_ce, total_halt, total_em, total_steps = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device, non_blocking=True)\n",
    "        y_true   = y_true.to(device,   non_blocking=True)\n",
    "\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "\n",
    "        for _ in range(N_SUP):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=None\n",
    "            )\n",
    "\n",
    "            # losses in float32\n",
    "            loss_ce = F.cross_entropy(logits.float().reshape(-1, OUTPUT_TOKENS), y_true.reshape(-1))\n",
    "            with torch.no_grad():\n",
    "                em = exact_match_from_logits(logits, y_true)\n",
    "            loss_halt = F.binary_cross_entropy_with_logits(halt_logit.float(), em)\n",
    "            loss = loss_ce + loss_halt\n",
    "            \n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)  # <-- add this\n",
    "            else:\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "                    \n",
    "\n",
    "            total_ce   += loss_ce.detach().item()\n",
    "            total_halt += loss_halt.detach().item()\n",
    "            total_em   += em.mean().item()\n",
    "            total_steps += 1\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | CE {total_ce/max(1,total_steps):.4f} | HaltBCE {total_halt/max(1,total_steps):.4f} | Exact-match {total_em/max(1,total_steps):.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: TRM, loader: DataLoader, n_sup_eval: int = N_SUP):\n",
    "    model.eval()\n",
    "    em_list, cell_acc_list = [], []\n",
    "\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device)\n",
    "        y_true   = y_true.to(device)\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "\n",
    "        for _ in range(n_sup_eval):\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=None  # make eval consistent\n",
    "            )\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        em = (preds == y_true).all(dim=1).float()\n",
    "        cell_acc = (preds == y_true).float().mean(dim=1)\n",
    "        em_list.append(em); cell_acc_list.append(cell_acc)\n",
    "\n",
    "    em = torch.cat(em_list).mean().item()\n",
    "    cell_acc = torch.cat(cell_acc_list).mean().item()\n",
    "    print(f\"Validation | Exact-match {em:.3f} | Cell accuracy {cell_acc:.3f}\")\n",
    "    return em, cell_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.0, betas=(0.9, 0.95))\n",
    "scaler = GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward-only finiteness: y1 True z1 True logits True halt_logit True\n",
      "Pre-backward finiteness: loss True loss_ce True loss_halt True\n",
      "Gradients finite: True\n",
      "Post-step forward finiteness: y2 True z2 True logits2 True halt2 True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# one batch\n",
    "x_tokens, y_true = next(iter(train_loader))\n",
    "x_tokens = x_tokens.to(device)\n",
    "y_true   = y_true.to(device)\n",
    "\n",
    "# forward-only check (no training, no AMP)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0, z0 = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "    y1, z1, logits, halt_logit = model.forward_step(x_tokens, y=y0, z=z0, n=N, T=T, k_last_ops=None)\n",
    "print(\"Forward-only finiteness:\",\n",
    "      \"y1\", torch.isfinite(y1).all().item(),\n",
    "      \"z1\", torch.isfinite(z1).all().item(),\n",
    "      \"logits\", torch.isfinite(logits).all().item(),\n",
    "      \"halt_logit\", torch.isfinite(halt_logit).all().item())\n",
    "\n",
    "# single training step in full FP32 (no AMP, tiny LR, no weight decay)\n",
    "model.train()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "opt.zero_grad(set_to_none=True)\n",
    "\n",
    "y0, z0 = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "y1, z1, logits, halt_logit = model.forward_step(x_tokens, y=y0, z=z0, n=N, T=T, k_last_ops=None)\n",
    "\n",
    "# compute losses in fp32\n",
    "loss_ce = F.cross_entropy(logits.float().reshape(-1, OUTPUT_TOKENS), y_true.reshape(-1))\n",
    "em = (logits.argmax(dim=-1) == y_true).all(dim=1).float()\n",
    "loss_halt = F.binary_cross_entropy_with_logits(halt_logit.float(), em)\n",
    "loss = loss_ce + loss_halt\n",
    "\n",
    "print(\"Pre-backward finiteness:\",\n",
    "      \"loss\", torch.isfinite(loss).item(),\n",
    "      \"loss_ce\", torch.isfinite(loss_ce).item(),\n",
    "      \"loss_halt\", torch.isfinite(loss_halt).item())\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# check grads\n",
    "all_grads_finite = True\n",
    "for n, p in model.named_parameters():\n",
    "    if p.grad is None:\n",
    "        continue\n",
    "    if not torch.isfinite(p.grad).all():\n",
    "        print(\"Non-finite grad in:\", n)\n",
    "        all_grads_finite = False\n",
    "        break\n",
    "print(\"Gradients finite:\", all_grads_finite)\n",
    "\n",
    "# clip and step\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "opt.step()\n",
    "\n",
    "# forward again after one small step\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0, z0 = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "    y2, z2, logits2, halt2 = model.forward_step(x_tokens, y=y0, z=z0, n=N, T=T, k_last_ops=None)\n",
    "print(\"Post-step forward finiteness:\",\n",
    "      \"y2\", torch.isfinite(y2).all().item(),\n",
    "      \"z2\", torch.isfinite(z2).all().item(),\n",
    "      \"logits2\", torch.isfinite(logits2).all().item(),\n",
    "      \"halt2\", torch.isfinite(halt2).all().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | CE 0.3838 | HaltBCE 0.3854 | Exact-match 0.271\n",
      "Validation | Exact-match 0.434 | Cell accuracy 0.909\n",
      "Validation | Exact-match 0.307 | Cell accuracy 0.894\n",
      "Validation (raw) | EM 0.434 | Cell 0.909\n",
      "Validation (EMA) | EM 0.307 | Cell 0.894\n",
      "Epoch 02 | CE 0.2264 | HaltBCE 0.4837 | Exact-match 0.482\n",
      "Validation | Exact-match 0.545 | Cell accuracy 0.927\n",
      "Validation | Exact-match 0.576 | Cell accuracy 0.935\n",
      "Validation (raw) | EM 0.545 | Cell 0.927\n",
      "Validation (EMA) | EM 0.576 | Cell 0.935\n",
      "Epoch 03 | CE 0.2160 | HaltBCE 0.4708 | Exact-match 0.541\n",
      "Validation | Exact-match 0.551 | Cell accuracy 0.924\n",
      "Validation | Exact-match 0.613 | Cell accuracy 0.935\n",
      "Validation (raw) | EM 0.551 | Cell 0.924\n",
      "Validation (EMA) | EM 0.613 | Cell 0.935\n",
      "Epoch 04 | CE 0.2093 | HaltBCE 0.4607 | Exact-match 0.562\n",
      "Validation | Exact-match 0.553 | Cell accuracy 0.921\n",
      "Validation | Exact-match 0.650 | Cell accuracy 0.938\n",
      "Validation (raw) | EM 0.553 | Cell 0.921\n",
      "Validation (EMA) | EM 0.650 | Cell 0.938\n",
      "Epoch 05 | CE 0.2108 | HaltBCE 0.4340 | Exact-match 0.584\n",
      "Validation | Exact-match 0.586 | Cell accuracy 0.924\n",
      "Validation | Exact-match 0.654 | Cell accuracy 0.936\n",
      "Validation (raw) | EM 0.586 | Cell 0.924\n",
      "Validation (EMA) | EM 0.654 | Cell 0.936\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_one_epoch(model, train_loader, optimizer, scaler, epoch, use_amp=False, ema=ema)\n",
    "\n",
    "    # Raw weights\n",
    "    em_raw, cell_raw = evaluate(model, val_loader, n_sup_eval=N_SUP)\n",
    "\n",
    "    # EMA weights\n",
    "    em_ema, cell_ema = evaluate_with_ema(model, ema, val_loader)  # uses N_SUP internally\n",
    "\n",
    "    print(f\"Validation (raw) | EM {em_raw:.3f} | Cell {cell_raw:.3f}\")\n",
    "    print(f\"Validation (EMA) | EM {em_ema:.3f} | Cell {cell_ema:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Puzzle 0:\n",
      "[[3 0 1 0]\n",
      " [0 0 0 2]\n",
      " [0 1 2 3]\n",
      " [2 0 0 1]]\n",
      "Pred:\n",
      "[[3 2 1 4]\n",
      " [1 4 3 2]\n",
      " [4 1 2 3]\n",
      " [2 3 4 1]]\n",
      "True:\n",
      "[[3 2 1 4]\n",
      " [1 4 3 2]\n",
      " [4 1 2 3]\n",
      " [2 3 4 1]]\n",
      "\n",
      "Puzzle 1:\n",
      "[[0 0 0 4]\n",
      " [4 0 3 1]\n",
      " [0 4 1 3]\n",
      " [0 1 4 2]]\n",
      "Pred:\n",
      "[[1 3 2 4]\n",
      " [4 2 3 1]\n",
      " [2 4 1 3]\n",
      " [3 1 4 2]]\n",
      "True:\n",
      "[[1 3 2 4]\n",
      " [4 2 3 1]\n",
      " [2 4 1 3]\n",
      " [3 1 4 2]]\n",
      "\n",
      "Puzzle 2:\n",
      "[[0 4 0 1]\n",
      " [0 0 3 4]\n",
      " [0 2 4 3]\n",
      " [4 3 1 0]]\n",
      "Pred:\n",
      "[[3 4 2 1]\n",
      " [2 1 3 4]\n",
      " [1 2 4 3]\n",
      " [4 3 1 2]]\n",
      "True:\n",
      "[[3 4 2 1]\n",
      " [2 1 3 4]\n",
      " [1 2 4 3]\n",
      " [4 3 1 2]]\n",
      "\n",
      "Puzzle 3:\n",
      "[[0 0 4 2]\n",
      " [0 4 3 1]\n",
      " [4 0 1 3]\n",
      " [3 0 0 4]]\n",
      "Pred:\n",
      "[[1 3 4 2]\n",
      " [2 4 3 1]\n",
      " [4 2 1 3]\n",
      " [3 1 2 4]]\n",
      "True:\n",
      "[[1 3 4 2]\n",
      " [2 4 3 1]\n",
      " [4 2 1 3]\n",
      " [3 1 2 4]]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def solve_and_show(model: TRM, loader: DataLoader, n_batches: int = 1):\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device)\n",
    "        y_true   = y_true.to(device)\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "        for _ in range(N_SUP):\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=None\n",
    "            )\n",
    "        preds_tok = logits.argmax(dim=-1).cpu().numpy()\n",
    "        xs = x_tokens.cpu().numpy()\n",
    "        ys_tok = y_true.cpu().numpy()\n",
    "        for i in range(min(4, xs.shape[0])):\n",
    "            print(f\"\\nPuzzle {shown+i}:\")\n",
    "            print(xs[i].reshape(4,4))\n",
    "            print(\"Pred:\")\n",
    "            print((preds_tok[i] + 1).reshape(4,4))   # tokens -> digits\n",
    "            print(\"True:\")\n",
    "            print((ys_tok[i] + 1).reshape(4,4))\n",
    "        shown += 1\n",
    "        if shown >= n_batches:\n",
    "            break\n",
    "\n",
    "solve_and_show(model, val_loader, n_batches=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Non-finite values detected at T0-hz0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# try on one batch\u001b[39;00m\n\u001b[32m     26\u001b[39m x_tokens, _ = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mforward_finiteness_probe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mforward_finiteness_probe\u001b[39m\u001b[34m(model, x_tokens)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     11\u001b[39m     h_z = (x_h + y + z) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model.cfg.stabilize_input_sums \u001b[38;5;28;01melse\u001b[39;00m (x_h + y + z) / math.sqrt(\u001b[32m3.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mT\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m-hz\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     z = model._net(h_z)\n\u001b[32m     14\u001b[39m     check(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mT\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-z\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, z)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mforward_finiteness_probe.<locals>.check\u001b[39m\u001b[34m(tag, t)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck\u001b[39m(tag, t):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.isfinite(t).all():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNon-finite values detected at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Non-finite values detected at T0-hz0"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def forward_finiteness_probe(model: TRM, x_tokens: torch.Tensor):\n",
    "    model.eval()\n",
    "    y, z = model.init_state(batch_size=x_tokens.size(0), device=x_tokens.device)\n",
    "    x_h = model.embed_input(x_tokens)\n",
    "    def check(tag, t):\n",
    "        if not torch.isfinite(t).all():\n",
    "            raise RuntimeError(f\"Non-finite values detected at {tag}\")\n",
    "    for t in range(T):\n",
    "        for i in range(N):\n",
    "            h_z = (x_h + y + z) if not model.cfg.stabilize_input_sums else (x_h + y + z) / math.sqrt(3.0)\n",
    "            check(f\"T{t}-hz{i}\", h_z)\n",
    "            z = model._net(h_z)\n",
    "            check(f\"T{t}-z{i}\", z)\n",
    "        h_y = (y + z) if not model.cfg.stabilize_input_sums else (y + z) / math.sqrt(2.0)\n",
    "        check(f\"T{t}-hy\", h_y)\n",
    "        y = model._net(h_y)\n",
    "        check(f\"T{t}-y\", y)\n",
    "    logits = model.output_head(y)\n",
    "    halt_logit = model.halt_head(y)\n",
    "    check(\"logits\", logits)\n",
    "    check(\"halt_logit\", halt_logit)\n",
    "    print(\"Forward finiteness probe passed.\")\n",
    "\n",
    "# try on one batch\n",
    "x_tokens, _ = next(iter(train_loader))\n",
    "forward_finiteness_probe(model, x_tokens.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tokens range: 0 4 torch.int64\n",
      "embedding finite? False max|w|: nan\n",
      "x_h finite? False\n"
     ]
    }
   ],
   "source": [
    "# one batch\n",
    "x_tokens, _ = next(iter(train_loader))\n",
    "x_tokens = x_tokens.to(device)\n",
    "\n",
    "# check input ids are in-range and integer\n",
    "print(\"x_tokens range:\", int(x_tokens.min()), int(x_tokens.max()), x_tokens.dtype)\n",
    "\n",
    "# check the embedding matrix\n",
    "w = model.input_emb.weight.data\n",
    "print(\"embedding finite?\", torch.isfinite(w).all().item(), \"max|w|:\", float(w.abs().max()))\n",
    "\n",
    "# check the immediate embedding lookup\n",
    "x_h = model.embed_input(x_tokens)\n",
    "print(\"x_h finite?\", torch.isfinite(x_h).all().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward finiteness probe passed.\n"
     ]
    }
   ],
   "source": [
    "x_tokens, _ = next(iter(train_loader))\n",
    "forward_finiteness_probe(model, x_tokens.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
