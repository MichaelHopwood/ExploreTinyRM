{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfded054",
   "metadata": {},
   "source": [
    "# Modular TRM Training for Community Detection\n",
    "\n",
    "This notebook demonstrates a modular approach to training and evaluating a TRM neural network for community detection on synthetic graphs using PyTorch. The code is organized for easy adaptation to other graph-based problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2aa4ce",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Set Up Environment\n",
    "Import all required libraries, set random seeds, and configure device (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7fe2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Any, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"..\", \"src\"))\n",
    "from exploretinyrm.trm import TRM, TRMConfig\n",
    "def set_seed(seed: int = 123):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9d301",
   "metadata": {},
   "source": [
    "## 2. AMP and EMA Utilities\n",
    "Define automatic mixed precision (AMP) and exponential moving average (EMA) utility functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd03503",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "    _USE_TORCH_AMP = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "    _USE_TORCH_AMP = False\n",
    "\n",
    "def make_grad_scaler(is_cuda: bool):\n",
    "    if _USE_TORCH_AMP:\n",
    "        try:\n",
    "            return _GradScaler(\"cuda\", enabled=is_cuda)\n",
    "        except TypeError:\n",
    "            return _GradScaler(enabled=is_cuda)\n",
    "    else:\n",
    "        return _GradScaler(enabled=is_cuda)\n",
    "\n",
    "def amp_autocast(is_cuda: bool, use_amp: bool):\n",
    "    if _USE_TORCH_AMP:\n",
    "        try:\n",
    "            return _autocast(device_type=\"cuda\", enabled=(is_cuda and use_amp))\n",
    "        except TypeError:\n",
    "            return _autocast(enabled=(is_cuda and use_amp))\n",
    "    else:\n",
    "        return _autocast(enabled=(is_cuda and use_amp))\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: torch.nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in model.named_parameters()\n",
    "            if param.requires_grad\n",
    "        }\n",
    "\n",
    "    def update(self, model: torch.nn.Module) -> None:\n",
    "        d = self.decay\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if not param.requires_grad:\n",
    "                    continue\n",
    "                self.shadow[name].mul_(d).add_(param.detach(), alpha=1.0 - d)\n",
    "\n",
    "    def copy_to(self, model: torch.nn.Module) -> None:\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in self.shadow:\n",
    "                    param.copy_(self.shadow[name])\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def use_ema_weights(model: torch.nn.Module, ema: EMA):\n",
    "    backup = {\n",
    "        name: param.detach().clone()\n",
    "        for name, param in model.named_parameters()\n",
    "        if param.requires_grad\n",
    "    }\n",
    "    ema.copy_to(model)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in backup:\n",
    "                    param.copy_(backup[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199c7f7",
   "metadata": {},
   "source": [
    "## 3. Community Detection Dataset Preparation\n",
    "Synthetic dataset for community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "548688a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "15440571 cannot be used to generate a random.Random instance",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     42\u001b[39m     ds_va = CommunityDetectionDataset(n_samples=n_val, n_nodes=n_nodes, n_communities=n_communities, p_in=p_in, p_out=p_out, seed=seed+\u001b[32m1\u001b[39m)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     44\u001b[39m         DataLoader(ds_tr, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     45\u001b[39m         DataLoader(ds_va, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     46\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m train_loader, val_loader = \u001b[43mget_gc_loaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_val\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#N_NODES\u001b[39;49;00m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_communities\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_in\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_out\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m123\u001b[39;49m\n\u001b[32m     57\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mget_gc_loaders\u001b[39m\u001b[34m(n_train, n_val, batch_size, n_nodes, n_communities, p_in, p_out, seed)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_gc_loaders\u001b[39m(n_train=\u001b[32m512\u001b[39m, n_val=\u001b[32m128\u001b[39m, batch_size=\u001b[32m16\u001b[39m, n_nodes=\u001b[32m30\u001b[39m, n_communities=\u001b[32m3\u001b[39m, p_in=\u001b[32m0.6\u001b[39m, p_out=\u001b[32m0.05\u001b[39m, seed=\u001b[32m42\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     ds_tr = \u001b[43mCommunityDetectionDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_communities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_communities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_in\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_out\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     ds_va = CommunityDetectionDataset(n_samples=n_val, n_nodes=n_nodes, n_communities=n_communities, p_in=p_in, p_out=p_out, seed=seed+\u001b[32m1\u001b[39m)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     44\u001b[39m         DataLoader(ds_tr, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     45\u001b[39m         DataLoader(ds_va, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     46\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mCommunityDetectionDataset.__init__\u001b[39m\u001b[34m(self, n_samples, n_nodes, n_communities, p_in, p_out, seed)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.p_out = p_out\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.rng = np.random.default_rng(seed)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28mself\u001b[39m.samples = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.p_out = p_out\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.rng = np.random.default_rng(seed)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28mself\u001b[39m.samples = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples)]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mCommunityDetectionDataset._generate_sample\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     28\u001b[39m probs = np.full((\u001b[38;5;28mself\u001b[39m.n_communities, \u001b[38;5;28mself\u001b[39m.n_communities), \u001b[38;5;28mself\u001b[39m.p_out)\n\u001b[32m     29\u001b[39m np.fill_diagonal(probs, \u001b[38;5;28mself\u001b[39m.p_in)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m G = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstochastic_block_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegers\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1e9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m labels = []\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sizes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhopw\\miniconda3\\envs\\torch121\\Lib\\site-packages\\networkx\\utils\\decorators.py:784\u001b[39m, in \u001b[36margmap.__call__.<locals>.func\u001b[39m\u001b[34m(_argmap__wrapper, *args, **kwargs)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(*args, __wrapper=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 5:3\u001b[39m, in \u001b[36margmap_stochastic_block_model_1\u001b[39m\u001b[34m(sizes, p, nodelist, seed, directed, selfloops, sparse, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbz2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhopw\\miniconda3\\envs\\torch121\\Lib\\site-packages\\networkx\\utils\\misc.py:486\u001b[39m, in \u001b[36mcreate_py_random_state\u001b[39m\u001b[34m(random_state)\u001b[39m\n\u001b[32m    483\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m PythonRandomInterface(random_state)\n\u001b[32m    485\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be used to generate a random.Random instance\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: 15440571 cannot be used to generate a random.Random instance"
     ]
    }
   ],
   "source": [
    "class GameDataset(Dataset):\n",
    "    \"\"\"Base class for game datasets. Subclass and implement _generate_sample.\"\"\"\n",
    "    def __init__(self, n_samples: int, seed: int = 0):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.samples = [self._generate_sample() for _ in range(n_samples)]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "    def _generate_sample(self): raise NotImplementedError()\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "class CommunityDetectionDataset(GameDataset):\n",
    "    \"\"\"Synthetic SBM community detection puzzles.\"\"\"\n",
    "    def __init__(self, n_samples: int, n_nodes: int = 30, n_communities: int = 3, p_in: float = 0.6, p_out: float = 0.05, seed: int = 0):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_communities = n_communities\n",
    "        self.p_in = p_in\n",
    "        self.p_out = p_out\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.samples = [self._generate_sample() for _ in range(n_samples)]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "    def _generate_sample(self):\n",
    "        sizes = [self.n_nodes // self.n_communities] * self.n_communities\n",
    "        for i in range(self.n_nodes % self.n_communities):\n",
    "            sizes[i] += 1\n",
    "        probs = np.full((self.n_communities, self.n_communities), self.p_out)\n",
    "        np.fill_diagonal(probs, self.p_in)\n",
    "        G = nx.stochastic_block_model(sizes, probs, seed=self.rng.integers(1e9))\n",
    "        labels = []\n",
    "        for idx, size in enumerate(sizes):\n",
    "            labels.extend([idx] * size)\n",
    "        labels = np.array(labels)\n",
    "        adj = nx.to_numpy_array(G)\n",
    "        x_tokens = adj.flatten().astype(np.float32)  # [n_nodes*n_nodes]\n",
    "        y_tokens = labels.astype(np.int64)           # [n_nodes]\n",
    "        return torch.from_numpy(x_tokens), torch.from_numpy(y_tokens)\n",
    "\n",
    "def get_gc_loaders(n_train=512, n_val=128, batch_size=16, n_nodes=30, n_communities=3, p_in=0.6, p_out=0.05, seed=42):\n",
    "    ds_tr = CommunityDetectionDataset(n_samples=n_train, n_nodes=n_nodes, n_communities=n_communities, p_in=p_in, p_out=p_out, seed=seed)\n",
    "    ds_va = CommunityDetectionDataset(n_samples=n_val, n_nodes=n_nodes, n_communities=n_communities, p_in=p_in, p_out=p_out, seed=seed+1)\n",
    "    return (\n",
    "        DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True),\n",
    "        DataLoader(ds_va, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    )\n",
    "\n",
    "train_loader, val_loader = get_gc_loaders(\n",
    "    n_train=2048,\n",
    "    n_val=512,\n",
    "    batch_size=16,\n",
    "    n_nodes=4, #N_NODES\n",
    "    n_communities=3,\n",
    "    p_in=0.6,\n",
    "    p_out=0.05,\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NODES = 30\n",
    "N_COMMUNITIES = 3\n",
    "INPUT_TOKENS = 2  # adjacency values: 0 or 1 (float)\n",
    "OUTPUT_TOKENS = N_COMMUNITIES\n",
    "SEQ_LEN = N_NODES\n",
    "\n",
    "D_MODEL = 128\n",
    "N_SUP = 16\n",
    "N = 6\n",
    "T = 3\n",
    "USE_ATT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379c6b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# show some examples of the dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     x, y = \u001b[43mtrain_loader\u001b[49m.dataset[i]\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInput Adjacency Matrix:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# show some examples of the dataset\n",
    "for i in range(2):\n",
    "    x, y = train_loader.dataset[i]\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Input Adjacency Matrix:\")\n",
    "    adj_matrix = x.numpy().reshape(N_NODES, N_NODES)\n",
    "    print(adj_matrix)\n",
    "    print(\"Node Colors:\")\n",
    "    print(y.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b85efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d6987",
   "metadata": {},
   "source": [
    "## 3. TRM Model Configuration for Community Detection\n",
    "Define the input/output vocabularies, sequence encoding, and instantiate the TRM model for node classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beef65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = TRMConfig(\n",
    "    input_vocab_size=INPUT_TOKENS,\n",
    "    output_vocab_size=OUTPUT_TOKENS,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=2,\n",
    "    use_attention=USE_ATT,\n",
    "    n_heads=8,\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=N,\n",
    "    T=T,\n",
    "    k_last_ops=None,\n",
    "    stabilize_input_sums=True\n",
    ")\n",
    "\n",
    "model = TRM(cfg).to(device)\n",
    "print(\"Params (M):\", sum(p.numel() for p in model.parameters())/1e6)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=3e-4, weight_decay=0.0, betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "scaler = make_grad_scaler(device.type == \"cuda\")\n",
    "ema = EMA(model, decay=0.999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54742356",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "Train the TRM model to predict community labels from the adjacency matrix, using permutation-invariant loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "def token_ce_loss(logits: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, V = logits.shape\n",
    "    return F.cross_entropy(logits.reshape(B*L, V), y_true.reshape(B*L))\n",
    "\n",
    "from src.exploretinyrm.utils import permutation_invariant_accuracy\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, epoch, use_amp=True, ema=None):\n",
    "    model.train()\n",
    "    total_ce, total_em, total_steps = 0.0, 0.0, 0\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device, non_blocking=True)\n",
    "        y_true   = y_true.to(device,   non_blocking=True)\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "        for _ in range(N_SUP):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=None\n",
    "            )\n",
    "            loss_ce = token_ce_loss(logits.float(), y_true)\n",
    "            with torch.no_grad():\n",
    "                preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "                em = permutation_invariant_accuracy(preds, y_true.cpu().numpy())\n",
    "            loss = loss_ce\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "            total_ce   += loss_ce.detach().item()\n",
    "            total_em   += em\n",
    "            total_steps += 1\n",
    "    print(f\"Epoch {epoch:02d} | CE {total_ce/max(1,total_steps):.4f} | Perm-Invariant Acc {total_em/max(1,total_steps):.3f}\")\n",
    "\n",
    "EPOCHS = 2\n",
    "node_acc_history = []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_one_epoch(model, train_loader, optimizer, scaler, epoch, use_amp=False)\n",
    "    # Optionally add validation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ad138",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Visualization\n",
    "Evaluate the trained TRM model on synthetic graphs and visualize the detected communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00190272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation and Visualization ---\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    em_list, node_acc_list = [], []\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device)\n",
    "        y_true   = y_true.to(device)\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "        for _ in range(N_SUP):\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=None\n",
    "            )\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "        em = permutation_invariant_accuracy(preds, y_true.cpu().numpy())\n",
    "        node_acc = (preds == y_true.cpu().numpy()).mean()\n",
    "        em_list.append(em)\n",
    "        node_acc_list.append(node_acc)\n",
    "    em = np.mean(em_list)\n",
    "    node_acc = np.mean(node_acc_list)\n",
    "    print(f\"Validation | Perm-Invariant Acc {em:.3f} | Node accuracy {node_acc:.3f}\")\n",
    "    return em, node_acc\n",
    "\n",
    "em, node_acc = evaluate(model, val_loader)\n",
    "print(\"Node accuracy history:\", node_acc_history)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "G, labels = CommunityDetectionDataset(n_samples=1, n_nodes=N_NODES, n_communities=N_COMMUNITIES, seed=999)[0]\n",
    "adj = G.numpy().reshape(N_NODES, N_NODES)\n",
    "G_nx = nx.from_numpy_array(adj)\n",
    "pos = nx.spring_layout(G_nx, seed=42)\n",
    "preds = model(torch.from_numpy(adj.flatten()).unsqueeze(0).to(device)).argmax(dim=-1).cpu().numpy()[0]\n",
    "nx.draw_networkx_nodes(G_nx, pos, node_color=preds, cmap=plt.cm.Set1, node_size=100)\n",
    "nx.draw_networkx_edges(G_nx, pos, alpha=0.5)\n",
    "plt.title(\"TRM Detected Communities\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
