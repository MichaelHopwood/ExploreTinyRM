{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfded054",
   "metadata": {},
   "source": [
    "# Modular TRM Training for Community Detection\n",
    "\n",
    "This notebook demonstrates a modular approach to training and evaluating a TRM neural network for community detection on synthetic graphs using PyTorch. The code is organized for easy adaptation to other graph-based problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2aa4ce",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Set Up Environment\n",
    "Import all required libraries, set random seeds, and configure device (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b7fe2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Any, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"..\", \"src\"))\n",
    "from exploretinyrm.trm import TRM, TRMConfig\n",
    "def set_seed(seed: int = 123):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9d301",
   "metadata": {},
   "source": [
    "## 2. AMP and EMA Utilities\n",
    "Define automatic mixed precision (AMP) and exponential moving average (EMA) utility functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ffd03503",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "    _USE_TORCH_AMP = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "    _USE_TORCH_AMP = False\n",
    "\n",
    "def make_grad_scaler(is_cuda: bool):\n",
    "    if _USE_TORCH_AMP:\n",
    "        try:\n",
    "            return _GradScaler(\"cuda\", enabled=is_cuda)\n",
    "        except TypeError:\n",
    "            return _GradScaler(enabled=is_cuda)\n",
    "    else:\n",
    "        return _GradScaler(enabled=is_cuda)\n",
    "\n",
    "def amp_autocast(is_cuda: bool, use_amp: bool):\n",
    "    if _USE_TORCH_AMP:\n",
    "        try:\n",
    "            return _autocast(device_type=\"cuda\", enabled=(is_cuda and use_amp))\n",
    "        except TypeError:\n",
    "            return _autocast(enabled=(is_cuda and use_amp))\n",
    "    else:\n",
    "        return _autocast(enabled=(is_cuda and use_amp))\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: torch.nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in model.named_parameters()\n",
    "            if param.requires_grad\n",
    "        }\n",
    "\n",
    "    def update(self, model: torch.nn.Module) -> None:\n",
    "        d = self.decay\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if not param.requires_grad:\n",
    "                    continue\n",
    "                self.shadow[name].mul_(d).add_(param.detach(), alpha=1.0 - d)\n",
    "\n",
    "    def copy_to(self, model: torch.nn.Module) -> None:\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in self.shadow:\n",
    "                    param.copy_(self.shadow[name])\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def use_ema_weights(model: torch.nn.Module, ema: EMA):\n",
    "    backup = {\n",
    "        name: param.detach().clone()\n",
    "        for name, param in model.named_parameters()\n",
    "        if param.requires_grad\n",
    "    }\n",
    "    ema.copy_to(model)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in backup:\n",
    "                    param.copy_(backup[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199c7f7",
   "metadata": {},
   "source": [
    "## 3. Community Detection Dataset Preparation\n",
    "Synthetic dataset for community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "548688a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameDataset(Dataset):\n",
    "    \"\"\"Base class for game datasets. Subclass and implement _generate_sample.\"\"\"\n",
    "    def __init__(self, n_samples: int, seed: int = 0):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.samples = [self._generate_sample() for _ in range(n_samples)]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "    def _generate_sample(self): raise NotImplementedError()\n",
    "\n",
    "class CommunityDetectionDataset(GameDataset):\n",
    "    \"\"\"Synthetic SBM community detection puzzles.\"\"\"\n",
    "    def __init__(self, n_samples: int, n_nodes: int = 30, n_communities: int = 3, p_in: float = 0.6, p_out: float = 0.05, seed: int = 0):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_communities = n_communities\n",
    "        self.p_in = p_in\n",
    "        self.p_out = p_out\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.samples = [self._generate_sample() for _ in range(n_samples)]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "    def _generate_sample(self):\n",
    "        sizes = [self.n_nodes // self.n_communities] * self.n_communities\n",
    "        for i in range(self.n_nodes % self.n_communities):\n",
    "            sizes[i] += 1\n",
    "        probs = np.full((self.n_communities, self.n_communities), self.p_out)\n",
    "        np.fill_diagonal(probs, self.p_in)\n",
    "        G = nx.stochastic_block_model(sizes, probs, seed=int(self.rng.integers(1e9)))\n",
    "        labels = []\n",
    "        for idx, size in enumerate(sizes):\n",
    "            labels.extend([idx] * size)\n",
    "        labels = np.array(labels)\n",
    "        adj = nx.to_numpy_array(G)\n",
    "        x_tokens = torch.from_numpy(adj.astype(np.int64))  # [n_nodes, n_nodes]\n",
    "        y_tokens = torch.from_numpy(labels.astype(np.int64))  # [n_nodes]\n",
    "        return x_tokens, y_tokens\n",
    "\n",
    "def get_gc_loaders(n_train=512, n_val=128, batch_size=16, n_nodes=30, n_communities=3, p_in=0.6, p_out=0.05, seed=42):\n",
    "    ds_tr = CommunityDetectionDataset(n_samples=n_train, n_nodes=n_nodes, n_communities=n_communities, p_in=p_in, p_out=p_out, seed=seed)\n",
    "    ds_va = CommunityDetectionDataset(n_samples=n_val, n_nodes=n_nodes, n_communities=n_communities, p_in=p_in, p_out=p_out, seed=seed+1)\n",
    "    return (\n",
    "        DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True),\n",
    "        DataLoader(ds_va, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    )\n",
    "\n",
    "train_loader, val_loader = get_gc_loaders(\n",
    "    n_train=2048,\n",
    "    n_val=512,\n",
    "    batch_size=16,\n",
    "    n_nodes=4, #N_NODES\n",
    "    n_communities=3,\n",
    "    p_in=0.6,\n",
    "    p_out=0.05,\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d39f7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "torch.Size([4, 4]) torch.Size([4])\n",
      "Example 1:\n",
      "torch.Size([4, 4]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# show some examples of the dataset\n",
    "for i in range(2):\n",
    "    x, y = train_loader.dataset[i]\n",
    "    print(f\"Example {i}:\")\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbcd8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NODES = 4\n",
    "N_COMMUNITIES = 3\n",
    "INPUT_TOKENS = 2  # adjacency values: 0 or 1 (float)\n",
    "OUTPUT_TOKENS = N_COMMUNITIES\n",
    "SEQ_LEN = N_NODES\n",
    "\n",
    "D_MODEL = 128\n",
    "N_SUP = 16\n",
    "N = 6\n",
    "T = 3\n",
    "USE_ATT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c379c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Input Adjacency Matrix:\n",
      "[[0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n",
      "Node Colors:\n",
      "[0 0 1 2]\n",
      "\n",
      "Example 1:\n",
      "Input Adjacency Matrix:\n",
      "[[0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "Node Colors:\n",
      "[0 0 1 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show some examples of the dataset\n",
    "for i in range(2):\n",
    "    x, y = train_loader.dataset[i]\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Input Adjacency Matrix:\")\n",
    "    adj_matrix = x.numpy().reshape(N_NODES, N_NODES)\n",
    "    print(adj_matrix)\n",
    "    print(\"Node Colors:\")\n",
    "    print(y.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d6987",
   "metadata": {},
   "source": [
    "## 4. Model Configuration for Community Detection\n",
    "Define the input/output vocabularies, sequence encoding, and instantiate the TRM model for node classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6beef65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params (M): 0.39488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = TRMConfig(\n",
    "    input_vocab_size=INPUT_TOKENS,\n",
    "    output_vocab_size=OUTPUT_TOKENS,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=2,\n",
    "    use_attention=USE_ATT,\n",
    "    n_heads=8,\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=N,\n",
    "    T=T,\n",
    "    k_last_ops=None,\n",
    "    stabilize_input_sums=True\n",
    ")\n",
    "\n",
    "model = TRM(cfg).to(device)\n",
    "print(\"Params (M):\", sum(p.numel() for p in model.parameters())/1e6)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=3e-4, weight_decay=0.0, betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "scaler = make_grad_scaler(device.type == \"cuda\")\n",
    "ema = EMA(model, decay=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54742356",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "Train the TRM model to predict community labels from the adjacency matrix, using permutation-invariant loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93cc71",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m node_acc_history = []\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# Optionally add validation here\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, scaler, epoch, use_amp, ema)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_SUP):\n\u001b[32m     14\u001b[39m     optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     y_state, z_state, logits, halt_logit = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     loss_ce = token_ce_loss(logits.float(), y_true)\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhopw\\Documents\\Work\\HRM\\ExploreTinyRM\\Alex\\..\\src\\exploretinyrm\\trm.py:437\u001b[39m, in \u001b[36mTRM.forward_step\u001b[39m\u001b[34m(self, x_tokens, y, z, n, T, k_last_ops)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    436\u001b[39m     y, z = \u001b[38;5;28mself\u001b[39m.init_state(batch_size=x_tokens.size(\u001b[32m0\u001b[39m), device=x_tokens.device)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeep_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhopw\\Documents\\Work\\HRM\\ExploreTinyRM\\Alex\\..\\src\\exploretinyrm\\trm.py:397\u001b[39m, in \u001b[36mTRM.deep_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, T, k_last_ops)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, T - \u001b[32m1\u001b[39m)):\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m         y, z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlatent_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# Final loop with gradients; optionally backprop only through the last k ops\u001b[39;00m\n\u001b[32m    400\u001b[39m y, z = \u001b[38;5;28mself\u001b[39m.latent_recursion(x_h, y, z, n=n, k_last_ops=k_last_ops, track_grads=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhopw\\Documents\\Work\\HRM\\ExploreTinyRM\\Alex\\..\\src\\exploretinyrm\\trm.py:356\u001b[39m, in \u001b[36mTRM.latent_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, k_last_ops, track_grads)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# n times: z = net(x + y + z)\u001b[39;00m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     h_z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sum_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstabilize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstabilize_input_sums\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m     ctx = nullcontext() \u001b[38;5;28;01mif\u001b[39;00m (track_grads \u001b[38;5;129;01mand\u001b[39;00m op_index >= cutoff) \u001b[38;5;28;01melse\u001b[39;00m torch.no_grad()\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhopw\\Documents\\Work\\HRM\\ExploreTinyRM\\Alex\\..\\src\\exploretinyrm\\trm.py:322\u001b[39m, in \u001b[36mTRM._sum_inputs\u001b[39m\u001b[34m(tensors, stabilize)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_inputs\u001b[39m(tensors: List[torch.Tensor], stabilize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     h = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stabilize:\n\u001b[32m    324\u001b[39m         h = h / math.sqrt(\u001b[38;5;28mlen\u001b[39m(tensors))\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "def token_ce_loss(logits: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, V = logits.shape\n",
    "    return F.cross_entropy(logits.reshape(B*L, V), y_true.reshape(B*L))\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, epoch, use_amp=True, ema=None):\n",
    "    model.train()\n",
    "    total_ce, total_acc, total_steps = 0.0, 0.0, 0\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device, non_blocking=True).long()\n",
    "        y_true   = y_true.to(device,   non_blocking=True)\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "        for _ in range(N_SUP):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=None\n",
    "            )\n",
    "            loss_ce = token_ce_loss(logits.float(), y_true)\n",
    "            with torch.no_grad():\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                acc = (preds == y_true).float().mean().item()\n",
    "            loss = loss_ce\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "            total_ce   += loss_ce.detach().item()\n",
    "            total_acc  += acc\n",
    "            total_steps += 1\n",
    "    print(f\"Epoch {epoch:02d} | CE {total_ce/max(1,total_steps):.4f} | Accuracy {total_acc/max(1,total_steps):.3f}\")\n",
    "\n",
    "EPOCHS = 2\n",
    "node_acc_history = []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_one_epoch(model, train_loader, optimizer, scaler, epoch, use_amp=False)\n",
    "    # Optionally add validation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ad138",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Visualization\n",
    "Evaluate the trained TRM model on synthetic graphs and visualize the detected communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00190272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation and Visualization ---\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    acc_list = []\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device).long()\n",
    "        y_true   = y_true.to(device)\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "        for _ in range(N_SUP):\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=None\n",
    "            )\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == y_true).float().mean().item()\n",
    "        acc_list.append(acc)\n",
    "    avg_acc = np.mean(acc_list)\n",
    "    print(f\"Validation | Accuracy {avg_acc:.3f}\")\n",
    "    return avg_acc\n",
    "\n",
    "avg_acc = evaluate(model, val_loader)\n",
    "print(\"Node accuracy history:\", node_acc_history)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "G, labels = CommunityDetectionDataset(n_samples=1, n_nodes=N_NODES, n_communities=N_COMMUNITIES, seed=999)[0]\n",
    "adj = G.numpy().reshape(N_NODES, N_NODES)\n",
    "G_nx = nx.from_numpy_array(adj)\n",
    "pos = nx.spring_layout(G_nx, seed=42)\n",
    "preds = model(torch.from_numpy(adj.flatten()).unsqueeze(0).to(device).long()).argmax(dim=-1).cpu().numpy()[0]\n",
    "nx.draw_networkx_nodes(G_nx, pos, node_color=preds, cmap=plt.cm.Set1, node_size=100)\n",
    "nx.draw_networkx_edges(G_nx, pos, alpha=0.5)\n",
    "plt.title(\"TRM Detected Communities\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
